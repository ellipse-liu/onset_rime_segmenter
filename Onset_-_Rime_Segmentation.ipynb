{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e27066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7773d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential issue with current dataset and training: onset + rime pair\n",
    "# what about cases of words that are only rime?\n",
    "# should it be considered to be a blank onset + rime or just rime\n",
    "\n",
    "# if there exists a rime whose pronunciation depends on the type or presence of onset, then blank onset is required\n",
    "\n",
    "# prep and clean data\n",
    "# matrix with width of three\n",
    "# index 0 = word\n",
    "# index 1 = onset\n",
    "# index 2 = rime\n",
    "word_file = open(\"split_words.txt\")\n",
    "\n",
    "word_list = word_file.readlines()\n",
    "word_list = [word.strip(\"\\n\") for word in word_list]\n",
    "random.shuffle(word_list)\n",
    "\n",
    "eng_list = np.empty(shape=(0,1))\n",
    "or_list = np.empty(shape=(0,4))\n",
    "\n",
    "for sets in word_list:\n",
    "    word, onset, rime = sets.split('\\t')\n",
    "    eng_list = np.vstack((eng_list, np.array([word])))\n",
    "    or_list = np.vstack((or_list, np.array(['<', onset, rime, '>'])))\n",
    "\n",
    "    \n",
    "# pairs = list(zip(eng_list,or_list))\n",
    "# random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c000d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries\n",
    "# first dictionary tokenizes full word into alphabetic character\n",
    "alpha_vocab = []\n",
    "onset_rime_vocab = []\n",
    "\n",
    "# this attempt, we  won't distinguish onsets and times categorically\n",
    "\n",
    "\n",
    "for word in eng_list:\n",
    "    for c in word:\n",
    "        if c not in alpha_vocab:\n",
    "            alpha_vocab += c\n",
    "            \n",
    "alpha_vocab = sorted(set(alpha_vocab))\n",
    "for e in or_list:\n",
    "    for c in e:\n",
    "        if c not in onset_rime_vocab:\n",
    "            onset_rime_vocab += [c]\n",
    "\n",
    "onset_rime_vocab = sorted(set(onset_rime_vocab))\n",
    "\n",
    "alpha_to_int = dict((a,i) for i,a in enumerate(alpha_vocab, 1))\n",
    "or_to_int = dict((a,i) for i,a in enumerate(onset_rime_vocab, 1))\n",
    "\n",
    "int_to_alpha = dict((i,a) for i,a in enumerate(alpha_vocab, 1))\n",
    "int_to_or = dict((i,a) for i,a in enumerate(onset_rime_vocab, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6333e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max len of encoder words\n",
    "\n",
    "arr_of_words = []\n",
    "\n",
    "for word in eng_list:\n",
    "    arr_of_words += [word[0]]\n",
    "    \n",
    "max_encoder_len = len(max(arr_of_words, key=len))\n",
    "max_decoder_len = 4\n",
    "num_encoder_vocab = len(alpha_vocab) + 1\n",
    "num_decoder_vocab = len(onset_rime_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4b6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x_train data\n",
    "x_tr = []\n",
    "for word in eng_list:\n",
    "    int_seq = []\n",
    "    for c in word[0]:\n",
    "        int_seq += [alpha_to_int[c]]\n",
    "    x_tr += [int_seq]\n",
    "x_tr = keras.preprocessing.sequence.pad_sequences(x_tr, maxlen=max_encoder_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11cab8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y_train data\n",
    "y_tr = []\n",
    "for seq in or_list:\n",
    "    int_seq = []\n",
    "    for c in seq:\n",
    "        int_seq += [or_to_int[c]]\n",
    "    y_tr += [int_seq]\n",
    "y_tr = keras.preprocessing.sequence.pad_sequences(y_tr, maxlen=max_decoder_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad54f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split_data\n",
    "# split_index = int(len(word_list) * .9)\n",
    "\n",
    "# y_test = y_tr[split_index:]\n",
    "# y_test_in = y_test[:, :-1]\n",
    "# y_test_out = y_test[:, 1:]\n",
    "\n",
    "# y_tr = y_tr[:split_index]\n",
    "y_tr_in = y_tr[:, :-1]\n",
    "y_tr_out = y_tr[:, 1:]\n",
    "\n",
    "# x_test = x_tr[split_index:]\n",
    "# x_tr = x_tr[:split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b40d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "embedding_dim = 200\n",
    "\n",
    "# three LSTM encoder model\n",
    "\n",
    "# define the encoder model\n",
    "encoder_inputs = keras.layers.Input(shape=(max_encoder_len, ))\n",
    "encoder_embed = keras.layers.Embedding(num_encoder_vocab, embedding_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "# first encoder LSTM\n",
    "encoder_LSTM1 = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_LSTM1(encoder_embed)\n",
    "\n",
    "# second encoder LSTM\n",
    "encoder_LSTM2 = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_LSTM2(encoder_output1) # encoder LSTMs feed into each other\n",
    "\n",
    "# third encoder LSTM\n",
    "encoder_LSTM3 = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output, state_h, state_c = encoder_LSTM3(encoder_output2) # final outputs and states to pass to decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aa04c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder LSTM model\n",
    "# input layer -> decoder embedding layer -> one LSTM layer -> Dense\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=(None,))\n",
    "\n",
    "# define layer architecture, then match to inputs\n",
    "decoder_embed_layer = keras.layers.Embedding(num_decoder_vocab, embedding_dim, trainable=True)\n",
    "decoder_embed = decoder_embed_layer(decoder_inputs)\n",
    "\n",
    "# decoder LSTM layer\n",
    "decoder_LSTM = keras.layers.LSTM(latent_dim, return_sequences=True, return_state= True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_LSTM(decoder_embed, initial_state=[state_h, state_c])\n",
    "\n",
    "# dense layer (output layer)\n",
    "# keras.layers.TimeDistributed layer considers temporal dimension\n",
    "# Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.\n",
    "decoder_dense = keras.layers.TimeDistributed(keras.layers.Dense(num_decoder_vocab, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = keras.models.Model([encoder_inputs,decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "798c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "\n",
    "Callbacks = [keras.callbacks.ModelCheckpoint(filepath='or_best_weights.h5', monitor='acc', verbose=2, save_best_only=True, mode='max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d73192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.9700 - acc: 0.3137\n",
      "Epoch 1: acc improved from -inf to 0.31375, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 13s 109ms/step - loss: 3.9700 - acc: 0.3137\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.1213 - acc: 0.3350\n",
      "Epoch 2: acc improved from 0.31375 to 0.33497, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 3.1213 - acc: 0.3350\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.9154 - acc: 0.3472\n",
      "Epoch 3: acc improved from 0.33497 to 0.34721, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.9154 - acc: 0.3472\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.7724 - acc: 0.3582\n",
      "Epoch 4: acc improved from 0.34721 to 0.35822, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 2.7724 - acc: 0.3582\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.6830 - acc: 0.3676\n",
      "Epoch 5: acc improved from 0.35822 to 0.36761, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 2.6830 - acc: 0.3676\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.5669 - acc: 0.3786\n",
      "Epoch 6: acc improved from 0.36761 to 0.37862, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 2.5669 - acc: 0.3786\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.4126 - acc: 0.4031\n",
      "Epoch 7: acc improved from 0.37862 to 0.40310, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 2.4126 - acc: 0.4031\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.2421 - acc: 0.4251\n",
      "Epoch 8: acc improved from 0.40310 to 0.42513, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.2421 - acc: 0.4251\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0999 - acc: 0.4500\n",
      "Epoch 9: acc improved from 0.42513 to 0.45002, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 2.0999 - acc: 0.4500\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9473 - acc: 0.4745\n",
      "Epoch 10: acc improved from 0.45002 to 0.47450, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 1.9473 - acc: 0.4745\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8462 - acc: 0.4937\n",
      "Epoch 11: acc improved from 0.47450 to 0.49368, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.8462 - acc: 0.4937\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7180 - acc: 0.5231\n",
      "Epoch 12: acc improved from 0.49368 to 0.52305, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 1.7180 - acc: 0.5231\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5924 - acc: 0.5414\n",
      "Epoch 13: acc improved from 0.52305 to 0.54141, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.5924 - acc: 0.5414\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5047 - acc: 0.5647\n",
      "Epoch 14: acc improved from 0.54141 to 0.56467, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.5047 - acc: 0.5647\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3968 - acc: 0.6014\n",
      "Epoch 15: acc improved from 0.56467 to 0.60139, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.3968 - acc: 0.6014\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3088 - acc: 0.6197\n",
      "Epoch 16: acc improved from 0.60139 to 0.61975, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 1.3088 - acc: 0.6197\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2222 - acc: 0.6573\n",
      "Epoch 17: acc improved from 0.61975 to 0.65728, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.2222 - acc: 0.6573\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1523 - acc: 0.6618\n",
      "Epoch 18: acc improved from 0.65728 to 0.66177, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.1523 - acc: 0.6618\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0638 - acc: 0.6854\n",
      "Epoch 19: acc improved from 0.66177 to 0.68543, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.0638 - acc: 0.6854\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9762 - acc: 0.7217\n",
      "Epoch 20: acc improved from 0.68543 to 0.72175, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.9762 - acc: 0.7217\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9203 - acc: 0.7377\n",
      "Epoch 21: acc improved from 0.72175 to 0.73766, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.9203 - acc: 0.7377\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8505 - acc: 0.7568\n",
      "Epoch 22: acc improved from 0.73766 to 0.75683, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.8505 - acc: 0.7568\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8098 - acc: 0.7744\n",
      "Epoch 23: acc improved from 0.75683 to 0.77438, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.8098 - acc: 0.7744\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7451 - acc: 0.7780\n",
      "Epoch 24: acc improved from 0.77438 to 0.77805, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.7451 - acc: 0.7780\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6822 - acc: 0.8029\n",
      "Epoch 25: acc improved from 0.77805 to 0.80294, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.6822 - acc: 0.8029\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6333 - acc: 0.8242\n",
      "Epoch 26: acc improved from 0.80294 to 0.82415, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.6333 - acc: 0.8242\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5738 - acc: 0.8474\n",
      "Epoch 27: acc improved from 0.82415 to 0.84741, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.5738 - acc: 0.8474\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5669 - acc: 0.8397\n",
      "Epoch 28: acc did not improve from 0.84741\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5669 - acc: 0.8397\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4946 - acc: 0.8694\n",
      "Epoch 29: acc improved from 0.84741 to 0.86944, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4946 - acc: 0.8694\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4519 - acc: 0.8829\n",
      "Epoch 30: acc improved from 0.86944 to 0.88290, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.4519 - acc: 0.8829\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4304 - acc: 0.8947\n",
      "Epoch 31: acc improved from 0.88290 to 0.89474, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.4304 - acc: 0.8947\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3781 - acc: 0.9078\n",
      "Epoch 32: acc improved from 0.89474 to 0.90779, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3781 - acc: 0.9078\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.3517 - acc: 0.9229\n",
      "Epoch 33: acc improved from 0.90779 to 0.92289, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3517 - acc: 0.9229\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3174 - acc: 0.9290\n",
      "Epoch 34: acc improved from 0.92289 to 0.92901, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.3174 - acc: 0.9290\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2906 - acc: 0.9392\n",
      "Epoch 35: acc improved from 0.92901 to 0.93921, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2906 - acc: 0.9392\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2608 - acc: 0.9478\n",
      "Epoch 36: acc improved from 0.93921 to 0.94778, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2608 - acc: 0.9478\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2354 - acc: 0.9572\n",
      "Epoch 37: acc improved from 0.94778 to 0.95716, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2354 - acc: 0.9572\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2149 - acc: 0.9527\n",
      "Epoch 38: acc did not improve from 0.95716\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2149 - acc: 0.9527\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1934 - acc: 0.9682\n",
      "Epoch 39: acc improved from 0.95716 to 0.96818, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.1934 - acc: 0.9682\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1811 - acc: 0.9661\n",
      "Epoch 40: acc did not improve from 0.96818\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1811 - acc: 0.9661\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1588 - acc: 0.9735\n",
      "Epoch 41: acc improved from 0.96818 to 0.97348, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.1588 - acc: 0.9735\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1457 - acc: 0.9763\n",
      "Epoch 42: acc improved from 0.97348 to 0.97634, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.1457 - acc: 0.9763\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1329 - acc: 0.9780\n",
      "Epoch 43: acc improved from 0.97634 to 0.97797, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.1329 - acc: 0.9780\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1124 - acc: 0.9849\n",
      "Epoch 44: acc improved from 0.97797 to 0.98490, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.1124 - acc: 0.9849\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1015 - acc: 0.9865\n",
      "Epoch 45: acc improved from 0.98490 to 0.98654, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1015 - acc: 0.9865\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0926 - acc: 0.9865\n",
      "Epoch 46: acc did not improve from 0.98654\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.0926 - acc: 0.9865\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0889 - acc: 0.9869\n",
      "Epoch 47: acc improved from 0.98654 to 0.98694, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.0889 - acc: 0.9869\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0724 - acc: 0.9927\n",
      "Epoch 48: acc improved from 0.98694 to 0.99266, saving model to or_best_weights.h5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.0724 - acc: 0.9927\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0721 - acc: 0.9906\n",
      "Epoch 49: acc did not improve from 0.99266\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0721 - acc: 0.9906\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0661 - acc: 0.9894\n",
      "Epoch 50: acc did not improve from 0.99266\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0661 - acc: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18d97be40d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_tr,y_tr_in], y_tr_out, epochs = 50, callbacks=Callbacks, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6700f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the inference model\n",
    "# load pretrained weights\n",
    "model = keras.models.load_model(\"or_best_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ddceb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder inference model\n",
    "encoder_model_i = keras.models.Model(inputs= encoder_inputs, outputs=[encoder_output, state_h, state_c])\n",
    "\n",
    "# decoder setup\n",
    "decoder_state_input_h = keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = keras.layers.Input(shape=(max_encoder_len, latent_dim))\n",
    "\n",
    "decoder_embed_i = decoder_embed_layer(decoder_inputs)\n",
    "\n",
    "decoder_output_i, state_h_i, state_c_i = decoder_LSTM(decoder_embed_i, initial_state = [decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "decoder_output_i = decoder_dense(decoder_output_i)\n",
    "\n",
    "# final decoder inference model\n",
    "decoder_model_i = keras.models.Model([decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c], [decoder_output_i] + [state_h_i, state_c_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55eb8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    e_out,e_h, e_c = encoder_model_i.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = or_to_int['<']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model_i.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = int_to_or[sampled_token_index]   \n",
    "        \n",
    "        if sampled_token != '>':\n",
    "            print(sampled_token)\n",
    "            decoded_sentence += [sampled_token]\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if (sampled_token == '>') or (len(decoded_sentence) >= max_decoder_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca9646da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2seq(input_word):\n",
    "    final_seq = []\n",
    "    for c in input_word:\n",
    "        final_seq += [alpha_to_int[c]]\n",
    "    final_seq = keras.preprocessing.sequence.pad_sequences([final_seq], maxlen=max_encoder_len, padding='post')[0]\n",
    "    return final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2840bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a single syllable word: crink\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "cr\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "ink\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "['cr', 'ink']\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a single syllable word: \")\n",
    "word_seq = word2seq(word).reshape(1, max_encoder_len)\n",
    "print(decode_sequence(word_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2a847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_Playground] *",
   "language": "python",
   "name": "conda-env-ML_Playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
